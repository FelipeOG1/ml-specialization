"""
Multifeature Linear Regression


n = num_features
Xj = jth feature
   (i)
Xj     = value of feature j on ith example

->(i)
X     = ith training example (vector of training example)

->(2)
X     = second vector of training example  


Model:  fw,b(x) = w1x1 + ..... wnxn + b
    
Cost Function
  ->
J(w,b)



"""

import numpy as np
import matplotlib.pyplot as plt

class MultipleLinearRegression:
    def __init__(self):
        self.x_train = np.array([
            [10, 20, 30],   
            [5,  15, 25],               
            [2,  4,  6],   
            [8,  16, 24]    
        ])
        self.w = np.array([1.0,2.5,-3.3])
        self.b = 4
        self.m,self.n = self.x_train.shape[0],self.x_train.shape[1]
        self.y_train = np.array([460.0,232.0,315.0,178.0])
        self.y_hat = np.dot(self.x_train,self.w) + self.b 
        self.learning_rate = 0.03
        

    def compute_w_derivate(self,index):return ((self.y_hat - self.y_train) * self.x_train[:,index]).sum() / self.m
    def compute_b_derivate(self):return ((self.y_hat - self.y_train)).sum() / self.m
    def gradient_descent(self):
        for i in range(self.n):
            self.w[i] = self.w[i] - self.learning_rate * self.compute_w_derivate(i)
        
        self.b = self.compute_b_derivate()
    def compute_cost_function(self):return ((self.y_hat - self.y_train)).sum() ** 2


    def __call__(self):
        self.gradient_descent()
        self.
        
   
        


print(MultipleLinearRegression().foo(1))

